<p><br />
As I begin the uphill trek to firmly understand The Math of Machine Learning, the first stop on my ascension is <strong>The Gradient</strong>. In cliche fashion, not only does <strong>The Gradient</strong> help to descend the treacherous curve of an Objective Function, it has also forced me (as the learner) to tumble to the mountainâ€™s base, and learn a few fundamentals. Recommended pre-requisites:</p>

<ol>
  <li>Using Derivatives to Find Minima &amp; Maxima (Calculus)</li>
  <li>Partial Derivatives (Multivariate Calculus)</li>
  <li>Basic Vector Operations (Linear Algebra)</li>
</ol>

<h3 id="the-importance">The Importance</h3>

<p><em>This series is written as I, the learner, writer and Software Engineer, complete a series of online Math classes to understand Machine Learning. Each post will start explaining why the chosen concept is significant in the ML space.</em></p>

<p><strong>The Gradient</strong> is significant primarily due to its use in Gradient Descent algorithms - the most popular flavor being Stochastic (fancy word for random) Gradient Descent (SGD). SGD is used in a wide gamut of models, including Logistic Regression and Neural Networks. When iterating to find ideal weights for a model, The Gradient is a vector that points in the direction of greatest slope in n-dimensional space. In other words, for a given model, it helps the model converge (quickly) on a local minimum.</p>

<h3 id="the-math">The Math</h3>
