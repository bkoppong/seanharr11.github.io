<!DOCTYPE html>
<html>
<head>
    <!-- [[! Document Settings ]] -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- [[! Page Meta ]] -->
    <title>Gradient Descent: High Learning Rates & Divergence</title>
    <meta name="description" content="The Laziest Programmer - Because someone else has already solved your problem." />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link rel="shortcut icon" href="/assets/images/favicon.ico" >

    <!-- [[! Styles'n'Scripts ]] -->
    <link rel="stylesheet" type="text/css" href="/assets/css/screen.css" />
    <link rel="stylesheet" type="text/css"
          href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
    <link rel="stylesheet" type="text/css" href="/assets/css/syntax.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/bootstrap-grid.css" />

    <!-- [[! highlight.js ]] -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
    <link rel="stylesheet" type="text/css" href="/assets/css/bootstrap.css" />
    <script>hljs.initHighlightingOnLoad();</script>

    <!-- [[! Ghost outputs important style and meta data with this tag ]] -->
        <link rel="canonical" href="" />
    <meta name="referrer" content="origin" />
    <link rel="next" href="/page2/" />

    <meta property="og:site_name" content="The Laziest Programmer" />
    <meta property="og:type" content="article" />
    <meta property='article:section' content="Technology">
    
    <meta property='article:tag' content="math-of-machine-learning">
    
    <meta property="og:title" content="Gradient Descent: High Learning Rates & Divergence" />
    <meta property="og:description" content="The Laziest Programmer - Because someone else has already solved your problem." />
    <meta property="og:url" content="http://thelaziestprogrammer.com" />
    <meta property="og:image" content="/assets/images/tlp_logo_og.png" />

    <script type="text/javascript"
        src="/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@thelaziestprogrammer" />
    <meta name="twitter:creator" content="@seanharr11" />
    <meta name="twitter:title" content="Gradient Descent: High Learning Rates & Divergence" />
    <meta name="twitter:description" content="The Laziest Programmer - Because someone else has already solved your problem." />
    <meta name="twitter:url" content="http://thelaziestprogrammer.com" />
    <meta name="twitter:image" content="http://thelaziestprogrammer.com/assets/images/tlp_logo_og.png" />


    <!--script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Website",
    "url": "",
    "image": "/assets/images/tlp_logo_dark.png",
    "description": "Because someone else has already solved your problem.",
    "name": "The Laziest Programmer",
    "
}
    </script-->

    <meta name="generator" content="Jekyll 3.0.0" />
    <link rel="alternate" type="application/rss+xml" title="The Laziest Programmer" href="/feed.xml" />


</head>
<body class="home-template nav-closed">

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        <li class="nav-home " role="presentation"><a href="/">Home</a></li>
        <li class="nav-home" role="presentation"><a href="#">Topics</a>
           <ul style="padding-top:9px;padding-bottom:0px"> 
             <li class="nav-fiction " role="presentation"><a href="/tag/databases">Databases</a></li>
           </ul>     
           <ul style="padding-top:0px; padding-bottom: 0px"> 
             <li class="nav-fiction " role="presentation"><a href="/tag/web-development">Web Development</a></li>
           </ul>     
           <ul style="padding-top:0px"> 
             <li class="nav-fiction " role="presentation"><a href="/tag/math-of-machine-learning">The Math of ML</a></li>
           </ul>     
        </li>
        <li class="nav-about " role="presentation"><a href="/about">About This Site</a></li>
        <li class="nav-author " role="presentation"><a href="/author/sharrington">The Creator</a></li>
    </ul>
    <a class="subscribe-button" href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=DH544PY7RFSLA">Donate</a>
    <a class="subscribe-button icon-feed" href="/feed.xml">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        <!-- [[! Everything else gets inserted here ]] -->
        <!-- < default -->

<!-- The comment above "< default" means - insert everything in this file into -->
    <!-- the [body] of the default.hbs template, which contains our header/footer. -->

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<header class="main-header post-head " style="background-image: url(/assets/images/banners/posts/math_of_ml/marin_valley.jpg) ">
    <nav class="main-nav  overlay  clearfix">
        <a class="blog-logo" href="/"><img src="/assets/images/tlp_logo.png" alt="Blog Logo" /></a>
        
            <a class="menu-button icon-menu" href="#"><span class="word">Menu</span></a>
        
    </nav>
</header>

<main class="content" role="main">

    <article class="post tag-math-of-machine-learning">

        <header class="post-header">
            <h1 class="post-title">Gradient Descent: High Learning Rates & Divergence</h1>
            <section class="post-meta">
            <!-- <a href=''>Sean Harrington</a> -->
            <time class="post-date" datetime="2017-07-01">01 Jul 2017</time>
                <!-- [[tags prefix=" on "]] -->
                 
                on 
                
                    
                       <a href='/tag/math-of-machine-learning'>Math-of-machine-learning</a>
                       
                
                
            </section>
        </header>

        <section class="post-content">
            
            <p><br />
We’ve <a href="/sharrington/math-of-machine-learning/the-gradient-a-visual-descent">explored gradient descent</a>, but we haven’t talked about learning rates, and how these hyperparameters are the key differentiators between convergence, and divergence.</p>

<p>More specifically, let’s <strong>visually</strong> explore what happens when we set our learning rate to be too high, and talk about strategies to avoid divergence.</p>

<h3 id="the-code">The Code</h3>

<p>As a refresher, in our example we are trying to minimize the Mean Squared Error (MSE) with gradient descent, by choosing ideal values for (\(a, b\)). Details can be found in the <a href="/sharrington/math-of-machine-learning/the-gradient-a-visual-descent">previous post on the gradient</a></p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c"># I've experimented w/ Python3.6 unicode symbols in this snippet</span>
<span class="k">def</span> <span class="nf">gradient_descent_mse</span><span class="p">(</span><span class="n">x_vec</span><span class="p">,</span> <span class="n">y_vec</span><span class="p">):</span>    
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_vec</span><span class="p">)</span>   
    <span class="c"># Define Partials &amp; Objective Function (MSE)</span>
    <span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>    
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>    
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="k">def</span> <span class="nf">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x_vec</span> <span class="o">*</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">))</span> <span class="o">/</span> <span class="n">n</span>    
    <span class="c"># Initialize our weights</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">250</span>    
    <span class="n">b</span> <span class="o">=</span> <span class="mi">250000</span>                                                                   
    <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Infinity</span>    
    <span class="n">j</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="c"># Convergence Conditions</span>
    <span class="err">δ</span> <span class="o">=</span> <span class="mi">1</span>    
    <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">150</span>
    <span class="c"># Learning Rates ()</span>
    <span class="err">ŋ</span><span class="n">_b</span> <span class="o">=</span> <span class="o">.</span><span class="mi">15</span>
    <span class="err">ŋ</span><span class="n">_a</span> <span class="o">=</span> <span class="o">.</span><span class="mo">00000005</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="err">Δ</span><span class="n">j</span><span class="p">)</span> <span class="o">&gt;</span> <span class="err">δ</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span>                                       
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>                  
        <span class="c"># Find the gradient at the point (a, b)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>
        <span class="c"># Multiply each partial deriv by its learning rate </span>
        <span class="err">Δ</span><span class="n">a</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_a</span>    
        <span class="err">Δ</span><span class="n">b</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_b</span>    
        <span class="c"># Update our weights                              </span>
        <span class="n">a</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">a</span>                                                            
        <span class="n">b</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">b</span>    
        <span class="c"># Update the error at each iteration    </span>
        <span class="n">j_new</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>    
        <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="n">j_new</span>    
        <span class="n">j</span> <span class="o">=</span> <span class="n">j_new</span> 
    <span class="k">print</span><span class="p">(</span><span class="s">"y = {0}x + {1}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">)))</span>
</code></pre>
</div>

<h3 id="plotting-our-gradient-descent">Plotting our Gradient Descent</h3>

<p>When we plot the iterations of our gradient descent algorithm with the above learning rates, we see that we are guided in the right direction by gradient descent:</p>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_good.png" alt="Gradient Descent: Good Learning Rates" /></p>

<p>Let’s now see what happens if we bump up the learning rate of our variable \(a\) by a factor of <strong>~10</strong>…</p>

<div class="highlighter-rouge"><pre class="highlight"><code>    <span class="err">ŋ</span><span class="n">_b</span> <span class="o">=</span> <span class="o">.</span><span class="mi">15</span>
    <span class="c">#ŋ_a =.00000005</span>
    <span class="err">ŋ</span><span class="n">_a</span> <span class="o">=</span> <span class="o">.</span><span class="mo">000000575</span>
</code></pre>
</div>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_bad.png" alt="Gradient Descent: Learning Rates that are too high" /></p>

<p>Huh? What happened here? Our z-axis has a range of \([0…8e+42]\); many magnitudes greater than our “good” graph, causing our “objective function surface” to disappear into oblivion. Additionally, our error is increasing with every iteration!</p>

<p>Let’s cut the number of iterations down from 150, to 7 to see what is actually going on:</p>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_bad_7_iterations.png" alt="Gradient Descent: Learning Rates that are too high, zoomed" /></p>

<p>Ok. So that makes a bit more sense. <strong>Here’s what’s happening</strong>:</p>

<ol>
  <li>We start at the white point in the “valley”, and calculate the gradient at that point.</li>
  <li>We multiply our learning rates by our gradient and move along this vector to our new point (the slightly greenish point to the left of the white point)
    <ul>
      <li><em>Because our learning rate was so high, combined with the magnitude of the gradient, we “jumped over” our local minimum.</em></li>
    </ul>
  </li>
  <li>We calculate our gradient at point 2, and make our next move, again, jumping over our local minimum
    <ul>
      <li><em>Our gradient at point 2 is even greater than the gradient at point 1!</em></li>
      <li><em>Our next step will again, jump over our valley, and we will rinse and repeat for eternity…</em></li>
    </ul>
  </li>
  <li>Due to the convex, valley-like curve of our objective function, as we continue to jump from side to side, the gradient at each jump grows higher. Our error increases quadratically with each “jump”, and our algorithm diverges to infinite error.</li>
</ol>

<p><em><strong>Note:</strong> Just the “valley-jumping” alone is a problem that needs fixing - it can lead to slow convergence, and worse, divergence. I’ve chosen this example with runaway quadratic ascension because it was an easy way to choose a diverging gradient descent.</em></p>

<h3 id="remedies">Remedies</h3>

<p>Of course, we can <strong>manually tweak</strong> our learning rates for each weight until we find that our model converges.</p>

<p>Or, naively, to prevent our “runaway gradient ascent”, we could implement a simple check for this case in our loop. If we gain error (rather than losing it), we can divide the deltas (<em>which get added to our weights</em>) by 2, until we see a drop in error. (<em>Line 8 below</em>)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="err">Δ</span><span class="n">j</span><span class="p">)</span> <span class="o">&gt;</span> <span class="err">δ</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span>                                       
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>                  
        <span class="c"># Find the gradient at the point (a, b)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>
        <span class="c"># Multiply each partial deriv by its learning rate </span>
        <span class="err">Δ</span><span class="n">a</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_a</span>    
        <span class="err">Δ</span><span class="n">b</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_b</span>    
        <span class="k">while</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="err">Δ</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="err">Δ</span><span class="n">b</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">j</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Error increased, decreasing LRs"</span><span class="p">)</span>
            <span class="err">Δ</span><span class="n">a</span> <span class="o">/=</span> <span class="mi">2</span>
            <span class="err">Δ</span><span class="n">b</span> <span class="o">/=</span> <span class="mi">2</span>
        <span class="c"># Update our weights                              </span>
        <span class="n">a</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">a</span>                                                            
        <span class="n">b</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">b</span>    
        <span class="c"># Update the error at each iteration    </span>
        <span class="n">j_new</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>    
        <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="n">j_new</span>    
        <span class="n">j</span> <span class="o">=</span> <span class="n">j_new</span>
</code></pre>
</div>

<p>In our specific case, the above works. Our plotted gradient descent looks as follows:</p>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_power_of_2_backoff.png" alt="Gradient Descent: Power of 2 Step Size Backoff" /></p>

<p>In a more general, higher-dimensional example, some techniques to set learning rates such that you avoid the problems of divergence and “valley-jumping” include:</p>

<ol>
  <li><strong>Momentum</strong> - Add an additional term to the weight update formula, which, in our “ball down the hill” analogy, will help to get our ball rolling by compounding past gradients.</li>
  <li><strong>Backtracking Line Search</strong> - Dynamically make <em>smart</em> choices for the learning rate at each iteration by taking a step <em>far enough</em> in the direction of the gradient, but not so far that we increase our error.</li>
  <li><strong>Stochastic Gradient Descent</strong> - A faster (and often better) optimization algorithm that calculates gradients from single (\(x, y\)) samples, rather than the entire batch. The additional <em>noise</em> can be of help here, as you may get an errant data point that kicks off your path down the valley, rather than our divergent model above.</li>
  <li><strong>Normalization of Data</strong> - Normalizing data creates a less <a href="http://www.ou.edu/class/che-design/che5480-13/Optimization%20of%20Chemical%20Processes-Chapter%204.pdf">elliptical contour</a>, and will influence our objective function to have a more <strong>concentric circle</strong> countour (<em>think a circular mountain base instead of an elliptical base!</em>).</li>
</ol>

<p>We will not cover these here for the brevity’s sake, but feel free to explore on your own.</p>


        </section>

        <footer class="post-footer">

            <!-- Everything inside the #author tags pulls data from the author -->
            <!-- #author-->

            
            <figure class="author-image">
                <a class="img" href="/author/sharrington" style="background-image: url(/assets/images/headshot.jpg)"><span class="hidden">'s Picture</span></a>
            </figure>
            

            <section class="author">
                <h4><a href="/author/sharrington">Sean Harrington</a></h4>
                
                
                    <p> I believe that cover bands should welcome guitarists in the audience on-stage, that Mat Cauthon would make any 14-book series enjoyable, and that college football players can evolve into decent software developers.</p>
                
                <div class="author-meta">
                    <span class="author-location icon-location"> Boston, MA</span> 
                    <span class="author-link icon-link"><a href="http://github.com/seanharr11"> github.com/seanharr11</a></span> 
                </div>
            </section>

            <!-- /author  -->

            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="http://twitter.com/share?text=Gradient Descent: High Learning Rates & Divergence&amp;url=http://thelaziestprogrammer.comsharringtonmath-of-machine-learninggradient-descent-learning-rate-too-high"
                    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://thelaziestprogrammer.comsharringtonmath-of-machine-learninggradient-descent-learning-rate-too-high"
                    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://thelaziestprogrammer.comsharringtonmath-of-machine-learninggradient-descent-learning-rate-too-high"
                   onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>
            
            <!-- Add Disqus Comments -->
            
            
        </footer>

    </article>

</main>

<aside class="read-next">

    <!-- [[! next_post ]] -->
    
        <a class="read-next-story " style="background-image: url(/assets/images/banners/posts/math_of_ml/st_john_panorama.jpg)" href="/sharrington/math-of-machine-learning/solving-logreg-newtons-method">
            <section class="post">
                <h2>Solving Logistic Regression with Newton's Method</h2>
                <p>In this post we introduce **Newton's Method**, and how it can be used to solve...</p>
            </section>
        </a>
    
    <!-- [[! /next_post ]] -->
    <!-- [[! prev_post ]] -->
    
        <a class="read-next-story prev " style="background-image: url(/assets/images/banners/posts/math_of_ml/jay_peak_ascending.jpg)" href="/sharrington/math-of-machine-learning/the-gradient-a-visual-descent">
            <section class="post">
                <h2>The Gradient: A Visual Descent</h2>
                <p>In this post I aim to visually, mathematically and programatically explain the gradient, and how...</p>
            </section>
        </a>
    
    <!-- [[! /prev_post ]] -->
</aside>

<!-- /post -->


        <footer class="site-footer clearfix">
          <section class="copyright"><a href="">The Laziest Programmer</a> &copy; 2017</section>
          <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> using <a href="https://github.com/biomadeira/jasper">Jasper</a></section>
        </footer>
    </div>
    <!-- [[! Ghost outputs important scripts and data with this tag ]] -->
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
    <!-- [[! The main JavaScript file for Casper ]] -->
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="/assets/js/index.js"></script>

    <!-- Add Google Analytics  -->
        <!-- Google Analytics Tracking code -->
     <script>
	    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	    ga('create', 'UA-80966627-1', 'auto');
	    ga('send', 'pageview');

     </script>   
</body>
</html>
