<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>thelaziestprogrammer.com</title>
   
   <link>http://thelaziestprogrammer.com</link>
   <description>Because someone else has already solved your problem.</description>
   <language>en-uk</language>
   <managingEditor> Sean Harrington</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Solving Logistic Regression with Newton's Method</title>
	  <link>/sharrington/math-of-machine-learning/solving-logreg-newtons-method</link>
	  <author>Sean Harrington</author>
	  <pubDate>2017-07-06T11:32:00-04:00</pubDate>
	  <guid>/sharrington/math-of-machine-learning/solving-logreg-newtons-method</guid>
	  <description><![CDATA[
	     <p><br />
In this post we introduce <strong>Newton’s Method</strong>, and how it can be used to solve <strong>Logistic Regression</strong>. Logistic Regression introduces the concept of the <strong>Log-Likelihood</strong> of the Bernoulli distribution, and covers a neat transformation called the <strong>sigmoid function</strong>.</p>

<p>We also introduce <strong>The Hessian</strong>, a square matrix of second-order partial derivatives, and how it is used in conjunction with <strong>The Gradient</strong> to implement Newton’s Method.</p>

<p>Similiar to the <a href="http://thelaziestprogrammer.com/sharrington/math-of-machine-learning/the-gradient-a-visual-descent">initial post covering Linear Regression and The Gradient</a>, we will explore Newton’s Method  visually, mathematically, and programatically with Python to understand how our math concepts translate to implementing a practical solution to the problem of binary classification: Logistic Regression.</p>

<p><strong>Suggested prior knowledge:</strong></p>

<ol>
  <li>Derivatives and the Chain Rule (<em>Calculus</em>)</li>
  <li>Partial Derivatives &amp; <a href="http://http://thelaziestprogrammer.com/sharringto
 n/math-of-machine-learning/the-gradient-a-visual-descent">The Gradient</a>(<em>Multivariate Calculus</em>)</li>
  <li>Basic Vector Operations (<em>Linear Algebra</em>)</li>
  <li>Basic Understanding of NumPy</li>
  <li>Basic Understanding of Independent Probability</li>
</ol>

<h3 id="the-data">The Data</h3>

<p>Our dataset is made up of South Boston real estate data, including the value of each home, and a (<em>boolean</em>) column indicating if that home <em>has more than 2 bathrooms</em>.</p>

<p><script type="math/tex">\hat{x} = HomeValue = \left\langle 550000.00, 600000.00, ... 578000.00\right\rangle^{T}</script>
<script type="math/tex">\hat{y} = MoreThan2Bathrooms = \left\langle 1, 0, 0, ... 1\right\rangle^{T}</script></p>

<p><img src="/assets/images/plotly/south_boston_bedroom_logreg_raw.png" alt="S. Boston Homes - Homes with more than 2 Bedrooms" /></p>

<h3 id="the-model">The Model</h3>

<p>We will be learning a <strong>Logistic Regression</strong> model, that will act as a <strong>binary classifier</strong> predicting whether or not a home has more than 2 bathroom, given its value (in dollars).</p>

<p>We still want to solve a linear combination of features &amp; weights, but we need to transform this linear combination by wrapping it in a function that is smooth, and has its range defined as \([0, 1]\) (<em>because we are trying to map our linear combination to a binary output of 0 or 1!</em>)</p>

<p>The logistic function, or <strong>sigmoid function</strong>, accomplishes all of these things:</p>

<script type="math/tex; mode=display">h(x) = 1 / (1 + e^{-z}),\ \ z = \theta_{1}x + \theta_{2}</script>

<p><strong>Note:</strong> <em>We add \(\theta_{2}\) to the exponent as “the intercept” to provide more flexibility; we have 1 dimension of data, <strong>house_value</strong>, but we are solving a 2-dimensional model!</em></p>

<p><img src="/assets/images/sigmoid_function.png" alt="Sigmoid Function" /></p>

<p>Similiar to the way we defined our <strong>sum of squares</strong> objective function for linear regression, we want to use our hypothesis function, \(h(x)\), and formulate our <strong>likelihood function</strong> to maximize and fit our weights. <strong>Here’s where things get mathy:</strong></p>

<h3 id="the-math-defining-a-likelihood-function">The Math: Defining a Likelihood Function</h3>

<p>First we need to define a <a href="https://en.wikipedia.org/wiki/Probability_mass_function">Probability Mass Function</a>:</p>

<script type="math/tex; mode=display">P(y=1|\ x;\theta) = h_{\theta}(x)</script>

<script type="math/tex; mode=display">P(y=0|\ x;\theta) = 1 - h_{\theta}(x)</script>

<p><strong>Note:</strong> <em>The left-hand side of the first statement reads “The probability that y equals 1, given a feature-vector \(x\), and the hypothesis function’s weights \(\theta\).” Our hypothesis function (right-hand-side) calculates this probability.</em></p>

<p>These two statements can be condensed into one:</p>

<script type="math/tex; mode=display">P(y|\ x;\theta) = h_{\theta}(x)^{y}(1-h_{\theta}(x))^{1-y}</script>

<p>The table below shows how incorrect predictions by our hypothesis function (i.e. \(h(x)=.25, y=1\)) are penalized by generating low values. It also helps to understand how we condense the two statements into one.</p>

<table>
  <tbody>
    <tr>
      <td>\(h(x)\)</td>
      <td>\(y=0\)</td>
      <td>\(y=1\)</td>
    </tr>
    <tr>
      <td>.25</td>
      <td><strong>.75</strong></td>
      <td><strong>.25</strong></td>
    </tr>
    <tr>
      <td>.5</td>
      <td><strong>.5</strong></td>
      <td><strong>.5</strong></td>
    </tr>
    <tr>
      <td>.75</td>
      <td><strong>.25</strong></td>
      <td><strong>.75</strong></td>
    </tr>
    <tr>
      <td>.9</td>
      <td><strong>.1</strong></td>
      <td><strong>.9</strong></td>
    </tr>
  </tbody>
</table>

<p>Naturally, we want to maximize the right-hand-side of the above statement, which happens to be our <strong>likelihood function</strong>.</p>

<p>I like to think of the <strong>likelihood function</strong> as “<em>the likelihood that our model will <strong>correctly</strong> predict any given \(y\) value, given its corresponding feature vector \(\hat{x}\)</em>”. It is, however, important to <a href="https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability">distinguish between probability and likelihood.</a>.</p>

<p>Now, we expand our <strong>likelihood</strong> function by applying it to every sample in our training data. We multiply each individual <strong>likelihood</strong> together to get the cumulative likelihood that our model is accurately predicting \(y\) values of our training data:</p>

<script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{n}p(y_{i}|x_{i};\theta)</script>

<script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{n}h_{\theta}(x_{i})^{y_{i}}(1-h_{\theta}(x_{i}))^{1-y_{i}}</script>

<p>Seeing that we are multiplying “\(n\)” likelihoods together (<em>all less than 1.0</em>), where \(n\) is the number of training samples, we are going to get a product of magnitude \(10^{-n}\). This is bad news! Eventually, we will <strong>run out of precision</strong>, and Python will turn our very small floats to \(0\).</p>

<p>Our solution is to take the \(log\) of our likelihood function:</p>

<script type="math/tex; mode=display">\ell(\theta) = log\ L(\theta)</script>

<script type="math/tex; mode=display">\ell(\theta) = \sum_{i=1}^{n}y_{i}\log(h_{\theta}(x_{i})) + (1-y_{i})\log(1-h_{\theta}(x_{i}))</script>

<p><strong>Note:</strong> <em>Remember that \(log(xy) = log(x) + log(y)\), and that: \(log(x)^{n} = nlog(x)\)</em>.</p>

<p>This is called the <strong>log-likelihood</strong> of our hypothesis function.</p>

<p>Remember, our hypothesis function penalizes bad predictions by generating small values, so we want to <strong>maximize the log-likelihood.</strong> The curve of our log-likelihood is shown below:</p>

<p><img src="/assets/images/plotly/log_likelihood_normalized.png" alt="Log-likelihood Curve" /></p>

<p><em><strong>Note:</strong> By taking the log of our function to derive the <strong>log-likelihood</strong>, we guarantee (as an added bonus) that our objective function is <strong>strictly concave</strong>, meaning there is 1 global maximum.</em></p>

<h3 id="the-math-newtons-method-with-one-variable">The Math: Newton’s Method with One Variable</h3>

<p>Before we maximize our log-likelihood, let’s introduce <strong>Newton’s Method</strong>.</p>

<p><strong>Newton’s Method</strong> is an iterative equation solver: it is an algorithm to find the roots of a polynomial function. In the simple, one-variable case, Newton’s Method is implemented as follows:</p>

<ol>
  <li><em>Find the tangent line to \(f(x)\) at point \((x_{n}, y_{n})\)</em>
    <ul>
      <li>\(y = f’(x_{n})(x - x_{n}) + f(x_{n})\)</li>
    </ul>
  </li>
  <li><em>Find the x-intercept of the tangent line, \(x_{n+1}\)</em>
    <ul>
      <li>\(0 = f’(x_{n})(x_{n+1} - x_{n}) + f(x_{n})\)</li>
      <li>\(-f(x_{n}) = f’(x_{n})(x_{n+1} - x_{n})\)</li>
      <li>\(x_{n+1} = x_{n} - \frac{f(x_{n})}{f’(x_{n})}\)</li>
    </ul>
  </li>
  <li><em>Find the \(y\) value at the x-intercept.</em>
    <ul>
      <li>\(y_{n+1} = f(x_{n+1})\)</li>
    </ul>
  </li>
  <li><em>If \(y_{n+1} - y_{n} \approx 0\):</em>
    <ul>
      <li><strong>return \(y_{n+1}\)</strong> because we’ve converged!</li>
    </ul>
  </li>
  <li><em>Else update point \((x_{n}, y_{n})\), and iterate</em>
    <ul>
      <li>\(x = x_{n+1}, y = y_{n+1}\), <strong>goto (1)</strong>.</li>
    </ul>
  </li>
</ol>

<p>The following .gif (from Wikipedia) helps to visualize this method:</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif" alt="Newton's Method .gif" /></p>

<p>If you understand the more-verbose algorithm above, you will see that this boils down to:</p>

<ol>
  <li><strong>Until \(x_{n} - x_{n+1} \approx 0 \):</strong>
    <ul>
      <li>\(x_{n+1} = x_{n} - \frac{f(x_{n})}{f’(x_{n})}\)</li>
    </ul>
  </li>
</ol>

<p>Anyone who passed high school Calculus may be able to understand the above. But how do we generalize to the multivariate, “n-dimensional” case?</p>

<h3 id="the-math-newtons-method-in-n-dimensions">The Math: Newton’s Method in N-dimensions</h3>

<p>Recall that in n-dimensions, we replace single-variable derivatives with a vector of partial derivatives called the <strong>gradient</strong>.</p>

<p><a href="http://thelaziestprogrammer.com/sharrington/math-of-machine-learning/the-gradient-a-visual-descent#the-math-finding-the-gradient">Review the gradient here</a> if this concept is fuzzy to you.</p>

<p>Thus, our update rule, in its multivariate form, becomes our vector of parameters \(\hat{x}\), minus the scalar \(f(\hat{x})\) multiplied by the inverse of the gradient vector:</p>

<script type="math/tex; mode=display">\hat{x}_{n+1} = \hat{x}_{n} - f(\hat{x}_{n})*\nabla f(\hat{x}_{n})^{-1}</script>

<p><strong>Note:</strong> <em>The \(f’(x_{n})\) in \(\frac{f(x_{n})}{f’(x_{n})}\) becomes \(\nabla f(\hat{x}_{n})^{-1}\) because we generalize the “reciprocal of the scalar,  \(f(x_{n})\)” to the multivariate case, replacing it with the “inverse of the gradient, \(\nabla f(\hat{x}_{n})^{-1}\)”.</em></p>

<h3 id="the-math-maximizing-log-likelihood-with-newtons-method">The Math: Maximizing Log-Likelihood with Newton’s Method</h3>

<p>Recall that we want to maximize the log-likelihood, \(\ell(\theta)\) of our hypothesis function, \(h_{\theta}(x)\)</p>

<p>To maximize our function, we want to find the partial derivatives of \(\ell(\theta)\), and set them equal to 0, and solve for \(\theta_{1}\) and \(\theta_{2}\) to find the <a href="https://en.wikipedia.org/wiki/Critical_point_(mathematics)#Application_to_optimization">critical point</a> of the partials. This critical point will be the <strong>max</strong> of our log-likelihood.</p>

<p><strong>Note:</strong> <em>Since the log-likelihood is strictly concave, we have one <strong>global max</strong>. This means that we only have 1 <strong>critical point</strong>, so our solution hashed out above is the only solution!</em></p>

<p>This should sound familiar. We are trying to find values for \(\theta_{1}\) and \(\theta_{2}\) that make the partial derivatives 0. We are finding the <em>“roots”</em> of the vector of partials (<em>the gradient</em>). We can use Newton’s Method to this! Recall the update step in Newton’s Method:</p>

<script type="math/tex; mode=display">\hat{x}_{n+1} = \hat{x}_{n} - f(\hat{x}_{n})\frac{1}{\nabla f(\hat{x}_{n})}</script>

<p>We can substitute \(f(\hat{x}_{n})\) with the gradient, \(\nabla \ell(\theta)\), leaving us with:</p>

<script type="math/tex; mode=display">\hat{x}_{n+1} = \hat{x}_{n} - \nabla \ell(\theta)\frac{1}{?}</script>

<p>What about the \(?\) term above? Intuition tells us that we need to take the derivative of the gradient vector, just like we previously took the derivative of \(f(\hat{x}_{n})\).</p>

<p>Enter <strong>The Hessian.</strong></p>

<h3 id="the-math-the-hessian">The Math: The Hessian</h3>

<p>Given our pre-requisite knowledge of <strong>Multivariate Calculus</strong>, we should know that to find the “<em>second-order</em>” partial derivatives of a function, we take the partial derivative of each first-order partial, with respect to each parameter. If we have \(n\) parameters, then we have \(n^{2}\) second-order partial derivatives.</p>

<p>Consequently, The Hessian is a square matrix of second-order partial derivatives of order \(n\ x\ n\).</p>

<p>In our case of 2 parameters, \((\theta_{1}, \theta_{2})\), our Hessian looks as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
H_{\ell(\hat{\theta})} = \begin{bmatrix}\begin{split}
\frac{\partial^{2}{\ell}}{\partial{\theta_{1}^{2}}} &
\frac{\partial^{2}{\ell}}{\partial{\theta_{1}}\partial{\theta_{2}}} \\
\frac{\partial^{2}{\ell}}{\partial{\theta_{2}}\partial{\theta_{1}}} &
\frac{\partial^{2}{\ell}}{\partial{\theta_{2}^{2}}} \\
\end{split}\end{bmatrix} %]]></script>

<h3 id="the-math-putting-it-all-together">The Math: Putting it all Together</h3>

<p>By substituting <strong>The Hessian</strong> into the <strong>Newton’s Method</strong> update step, we are left with:</p>

<script type="math/tex; mode=display">\theta_{n+1} = \theta_{n} + H_{\ell(\hat{\theta})}^{-1}\nabla \ell(\theta)</script>

<p><strong>Note:</strong> <em>We take the <strong>inverse</strong> of The Hessian, rather than taking its reciprocal because it is a matrix</em></p>

<p>For brevity’s sake, this post leaves out the actual derivation of the gradient and the hessian. Resources to understand the following derivations can be found at:</p>

<ol>
  <li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Derivation of the Gradient of our Log-Likelihood</a>, Andrew Ng’s lecture notes, pages 17-18.</li>
  <li>The derivation of the hessian is pretty straight forward, given Andrew Ng’s notes on the derivative of the <strong>sigmoid function</strong>, \(g’(z)\), once you’ve calculated the gradient.</li>
</ol>

<p>The <strong>gradient</strong> of \(\ell(\theta)\) is:</p>

<script type="math/tex; mode=display">\nabla \ell = \left\langle\matrix{
\sum_{i=1}^{n}(y_{i} - h_{\theta}(x_{i}))x_{i}\cr
\sum_{i=1}^{n}(y_{i} - h_{\theta}(x_{i}))
}\right\rangle</script>

<p>While the <strong>hessian</strong> of \(\ell(\theta)\) is:</p>

<script type="math/tex; mode=display">% <![CDATA[
H_{\ell(\hat{\theta})} = \begin{bmatrix}\begin{split}
\sum_{i=1}^{n}h_{\theta}(x_{i})(1-h_{\theta}(x_{i}))\theta_{1}\theta_{1},\ &
\sum_{i=1}^{n}h_{\theta}(x_{i})(1-h_{\theta}(x_{i}))\theta_{1}\\
\sum_{i=1}^{n}h_{\theta}(x_{i})(1-h_{\theta}(x_{i}))\theta_{1},\ &
\sum_{i=1}^{n}h_{\theta}(x_{i})(1-h_{\theta}(x_{i}))
\end{split}\end{bmatrix} %]]></script>

<p>Where \(h_{\theta}(x) = \frac{1}{1 + e^{-z}}\) and \(z = \theta_{1}x + \theta_{2}\).</p>

<h3 id="implementing-newtons-method">Implementing Newton’s Method</h3>

<p>We start off by defining our <strong>hypothesis function</strong>, which is the <strong>sigmoid function</strong>.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">):</span>                                                        
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="err">Θ</span><span class="n">_1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">"float_"</span><span class="p">)</span>                                              
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>                                                
</code></pre>
</div>

<p>Then we define \(\ell(\theta)\), our <strong>log-likelihood</strong> function:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">):</span>                                                                
    <span class="n">sigmoid_probs</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span>                                        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigmoid_probs</span><span class="p">)</span>
                  <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_probs</span><span class="p">))</span>                                     
</code></pre>
</div>

<p>Finally, we implement the <strong>gradient</strong> and the <strong>hessian</strong> of our log-likelihood.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">):</span>                                                         
    <span class="n">sigmoid_probs</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span>                                        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">sigmoid_probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">),</span>                          
                     <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">sigmoid_probs</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="p">)]])</span>                         

<span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">):</span>                                                          
    <span class="n">sigmoid_probs</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span>                                        
    <span class="n">d1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">sigmoid_probs</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_probs</span><span class="p">))</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>                  
    <span class="n">d2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">sigmoid_probs</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_probs</span><span class="p">))</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">1</span><span class="p">)</span>                  
    <span class="n">d3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">sigmoid_probs</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_probs</span><span class="p">))</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">1</span><span class="p">)</span>                  
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">],[</span><span class="n">d2</span><span class="p">,</span> <span class="n">d3</span><span class="p">]])</span>                                           
    <span class="k">return</span> <span class="n">H</span>
</code></pre>
</div>

<p>With all of our math implemented in these 4 functions, we can create the outer <em>while loop</em> and iterate using <strong>Newton’s Method</strong> until we converge on the maximum.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">newtons_method</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>                                                             
    <span class="s">"""
    :param x (np.array(float)): Vector of Boston House Values in dollars
    :param y (np.array(boolean)): Vector of Bools indicting if house has &gt; 2 bedrooms:
    :returns: np.array of logreg's parameters after convergence, [Θ_1, Θ_2]
    """</span>

    <span class="c"># Initialize log_likelihood &amp; parameters                                                                   </span>
    <span class="err">Θ</span><span class="n">_1</span> <span class="o">=</span> <span class="mf">15.1</span>                                                                     
    <span class="err">Θ</span><span class="n">_2</span> <span class="o">=</span> <span class="o">-.</span><span class="mi">4</span> <span class="c"># The intercept term                                                                 </span>
    <span class="err">Δ</span><span class="n">l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Infinity</span>                                                                
    <span class="n">l</span> <span class="o">=</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span>                                                                 
    <span class="c"># Convergence Conditions                                                        </span>
    <span class="err">δ</span> <span class="o">=</span> <span class="o">.</span><span class="mo">0000000001</span>                                                                 
    <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">15</span>                                                            
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>                                                                           
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="err">Δ</span><span class="n">l</span><span class="p">)</span> <span class="o">&gt;</span> <span class="err">δ</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span>                                       
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>                                                                      
        <span class="n">g</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span>                                                      
        <span class="n">hess</span> <span class="o">=</span> <span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span>                                                 
        <span class="n">H_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">hess</span><span class="p">)</span>                                                 
        <span class="c"># @ is syntactic sugar for np.dot(H_inv, g.T)¹</span>
        <span class="err">Δ</span> <span class="o">=</span> <span class="n">H_inv</span> <span class="err">@</span> <span class="n">g</span><span class="o">.</span><span class="n">T</span>                                                             
        <span class="err">ΔΘ</span><span class="n">_1</span> <span class="o">=</span> <span class="err">Δ</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>                                                              
        <span class="err">ΔΘ</span><span class="n">_2</span> <span class="o">=</span> <span class="err">Δ</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>                                                              
                                                                                    
        <span class="c"># Perform our update step                                                    </span>
        <span class="err">Θ</span><span class="n">_1</span> <span class="o">+=</span> <span class="err">ΔΘ</span><span class="n">_1</span>                                                                 
        <span class="err">Θ</span><span class="n">_2</span> <span class="o">+=</span> <span class="err">ΔΘ</span><span class="n">_2</span>                                                                 
                                                                                    
        <span class="c"># Update the log-likelihood at each iteration                                     </span>
        <span class="n">l_new</span> <span class="o">=</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">)</span>                                                      
        <span class="err">Δ</span><span class="n">l</span> <span class="o">=</span> <span class="n">l</span> <span class="o">-</span> <span class="n">l_new</span>                                                           
        <span class="n">l</span> <span class="o">=</span> <span class="n">l_new</span>                                                                
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="err">Θ</span><span class="n">_1</span><span class="p">,</span> <span class="err">Θ</span><span class="n">_2</span><span class="p">])</span>                                       
</code></pre>
</div>

<h3 id="visualizing-newtons-method">Visualizing Newton’s Method</h3>

<p>Let’s see what happens when we plot each iteration of Newton’s Method on our log-likelihood function’s surface:</p>

<p><strong>Note:</strong> <em>The first iteration is <em style="color: red">red</em>, the second iteration is <em style="color: orange">orange</em>….final iteration is <em style="color: purple">purple</em></em></p>

<p><img src="/assets/images/plotly/newtons_method_ll.png" alt="Newton's Method Logistic Regression" /></p>

<p>This graphic lends confirmation that our “purple” point is in fact the max, and that we have converged succesfully!</p>

<p><img src="/assets/images/plotly/newtons_method_max.png" alt="Newton's Method Maximum of LL" /></p>

<h3 id="visualizing-our-solution">Visualizing our Solution</h3>

<p>Normally, to visualize a 1-Dimensional dataset, you’d plot all of the points on a number-line, and place a boundary somewhere along the line. The problem here is that all of the data points get bunched together.</p>

<p>So instead, I’ve “stretched them out” along a y-axis, and labeled the points by color. We’ve also drawn 3 linear boundaries to separate the homes into percentiles – explained by the legend.</p>

<p><img src="/assets/images/plotly/south_boston_bedroom_logreg.png" alt="S. Boston Homes - Probability of Having &gt;2 Bedrooms" /></p>

<h3 id="conclusion">Conclusion</h3>

<p>we have covered a number of new topics, including <strong>The Hessian</strong>, <strong>Log-likelihood</strong> and <strong>The Sigmoid Function</strong>. Combining these methods together, we’ve been able implement <strong>Newton’s Method</strong> to solve <strong>Logistic Regression</strong>.</p>

<p>While these concepts enforce a <strong>very</strong> concrete foundation to implement our solution, we still need to be wary of things that can cause Newton’s Method to diverge. These are out of scope here, but you can read more on divergence here!</p>


	  ]]></description>
	</item>

	<item>
	  <title>Gradient Descent: High Learning Rates & Divergence</title>
	  <link>/sharrington/math-of-machine-learning/gradient-descent-learning-rate-too-high</link>
	  <author>Sean Harrington</author>
	  <pubDate>2017-07-01T03:26:00-04:00</pubDate>
	  <guid>/sharrington/math-of-machine-learning/gradient-descent-learning-rate-too-high</guid>
	  <description><![CDATA[
	     <p><br />
We’ve <a href="/sharrington/math-of-machine-learning/the-gradient-a-visual-descent">explored gradient descent</a>, but we haven’t talked about learning rates, and how these hyperparameters are the key differentiators between convergence, and divergence.</p>

<p>More specifically, let’s <strong>visually</strong> explore what happens when we set our learning rate to be too high, and talk about strategies to avoid divergence.</p>

<h3 id="the-code">The Code</h3>

<p>As a refresher, in our example we are trying to minimize the Mean Squared Error (MSE) with gradient descent, by choosing ideal values for (\(a, b\)). Details can be found in the <a href="/sharrington/math-of-machine-learning/the-gradient-a-visual-descent">previous post on the gradient</a></p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c"># I've experimented w/ Python3.6 unicode symbols in this snippet</span>
<span class="k">def</span> <span class="nf">gradient_descent_mse</span><span class="p">(</span><span class="n">x_vec</span><span class="p">,</span> <span class="n">y_vec</span><span class="p">):</span>    
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_vec</span><span class="p">)</span>   
    <span class="c"># Define Partials &amp; Objective Function (MSE)</span>
    <span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>    
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>    
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="k">def</span> <span class="nf">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x_vec</span> <span class="o">*</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">))</span> <span class="o">/</span> <span class="n">n</span>    
    <span class="c"># Initialize our weights</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">250</span>    
    <span class="n">b</span> <span class="o">=</span> <span class="mi">250000</span>                                                                   
    <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Infinity</span>    
    <span class="n">j</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="c"># Convergence Conditions</span>
    <span class="err">δ</span> <span class="o">=</span> <span class="mi">1</span>    
    <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">150</span>
    <span class="c"># Learning Rates ()</span>
    <span class="err">ŋ</span><span class="n">_b</span> <span class="o">=</span> <span class="o">.</span><span class="mi">15</span>
    <span class="err">ŋ</span><span class="n">_a</span> <span class="o">=</span> <span class="o">.</span><span class="mo">00000005</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="err">Δ</span><span class="n">j</span><span class="p">)</span> <span class="o">&gt;</span> <span class="err">δ</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span>                                       
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>                  
        <span class="c"># Find the gradient at the point (a, b)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>
        <span class="c"># Multiply each partial deriv by its learning rate </span>
        <span class="err">Δ</span><span class="n">a</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_a</span>    
        <span class="err">Δ</span><span class="n">b</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_b</span>    
        <span class="c"># Update our weights                              </span>
        <span class="n">a</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">a</span>                                                            
        <span class="n">b</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">b</span>    
        <span class="c"># Update the error at each iteration    </span>
        <span class="n">j_new</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>    
        <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="n">j_new</span>    
        <span class="n">j</span> <span class="o">=</span> <span class="n">j_new</span> 
    <span class="k">print</span><span class="p">(</span><span class="s">"y = {0}x + {1}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">)))</span>
</code></pre>
</div>

<h3 id="plotting-our-gradient-descent">Plotting our Gradient Descent</h3>

<p>When we plot the iterations of our gradient descent algorithm with the above learning rates, we see that we are guided in the right direction by gradient descent:</p>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_good.png" alt="Gradient Descent: Good Learning Rates" /></p>

<p>Let’s now see what happens if we bump up the learning rate of our variable \(a\) by a factor of <strong>~10</strong>…</p>

<div class="highlighter-rouge"><pre class="highlight"><code>    <span class="err">ŋ</span><span class="n">_b</span> <span class="o">=</span> <span class="o">.</span><span class="mi">15</span>
    <span class="c">#ŋ_a =.00000005</span>
    <span class="err">ŋ</span><span class="n">_a</span> <span class="o">=</span> <span class="o">.</span><span class="mo">000000575</span>
</code></pre>
</div>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_bad.png" alt="Gradient Descent: Learning Rates that are too high" /></p>

<p>Huh? What happened here? Our z-axis has a range of \([0…8e+42]\); many magnitudes greater than our “good” graph, causing our “objective function surface” to disappear into oblivion. Additionally, our error is increasing with every iteration!</p>

<p>Let’s cut the number of iterations down from 150, to 7 to see what is actually going on:</p>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_bad_7_iterations.png" alt="Gradient Descent: Learning Rates that are too high, zoomed" /></p>

<p>Ok. So that makes a bit more sense. <strong>Here’s what’s happening</strong>:</p>

<ol>
  <li>We start at the white point in the “valley”, and calculate the gradient at that point.</li>
  <li>We multiply our learning rates by our gradient and move along this vector to our new point (the slightly greenish point to the left of the white point)
    <ul>
      <li><em>Because our learning rate was so high, combined with the magnitude of the gradient, we “jumped over” our local minimum.</em></li>
    </ul>
  </li>
  <li>We calculate our gradient at point 2, and make our next move, again, jumping over our local minimum
    <ul>
      <li><em>Our gradient at point 2 is even greater than the gradient at point 1!</em></li>
      <li><em>Our next step will again, jump over our valley, and we will rinse and repeat for eternity…</em></li>
    </ul>
  </li>
  <li>Due to the convex, valley-like curve of our objective function, as we continue to jump from side to side, the gradient at each jump grows higher. Our error increases quadratically with each “jump”, and our algorithm diverges to infinite error.</li>
</ol>

<p><em><strong>Note:</strong> Just the “valley-jumping” alone is a problem that needs fixing - it can lead to slow convergence, and worse, divergence. I’ve chosen this example with runaway quadratic ascension because it was an easy way to choose a diverging gradient descent.</em></p>

<h3 id="remedies">Remedies</h3>

<p>Of course, we can <strong>manually tweak</strong> our learning rates for each weight until we find that our model converges.</p>

<p>Or, naively, to prevent our “runaway gradient ascent”, we could implement a simple check for this case in our loop. If we gain error (rather than losing it), we can divide the deltas (<em>which get added to our weights</em>) by 2, until we see a drop in error. (<em>Line 8 below</em>)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="err">Δ</span><span class="n">j</span><span class="p">)</span> <span class="o">&gt;</span> <span class="err">δ</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span>                                       
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>                  
        <span class="c"># Find the gradient at the point (a, b)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>
        <span class="c"># Multiply each partial deriv by its learning rate </span>
        <span class="err">Δ</span><span class="n">a</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_a</span>    
        <span class="err">Δ</span><span class="n">b</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_b</span>    
        <span class="k">while</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="err">Δ</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="err">Δ</span><span class="n">b</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">j</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Error increased, decreasing LRs"</span><span class="p">)</span>
            <span class="err">Δ</span><span class="n">a</span> <span class="o">/=</span> <span class="mi">2</span>
            <span class="err">Δ</span><span class="n">b</span> <span class="o">/=</span> <span class="mi">2</span>
        <span class="c"># Update our weights                              </span>
        <span class="n">a</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">a</span>                                                            
        <span class="n">b</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">b</span>    
        <span class="c"># Update the error at each iteration    </span>
        <span class="n">j_new</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>    
        <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="n">j_new</span>    
        <span class="n">j</span> <span class="o">=</span> <span class="n">j_new</span>
</code></pre>
</div>

<p>In our specific case, the above works. Our plotted gradient descent looks as follows:</p>

<p><img src="/assets/images/plotly/gd_learning_rates/gd_lr_power_of_2_backoff.png" alt="Gradient Descent: Power of 2 Step Size Backoff" /></p>

<p>In a more general, higher-dimensional example, some techniques to set learning rates such that you avoid the problems of divergence and “valley-jumping” include:</p>

<ol>
  <li><strong>Momentum</strong> - Add an additional term to the weight update formula, which, in our “ball down the hill” analogy, will help to get our ball rolling by compounding past gradients.</li>
  <li><strong>Backtracking Line Search</strong> - Dynamically make <em>smart</em> choices for the learning rate at each iteration by taking a step <em>far enough</em> in the direction of the gradient, but not so far that we increase our error.</li>
  <li><strong>Stochastic Gradient Descent</strong> - A faster (and often better) optimization algorithm that calculates gradients from single (\(x, y\)) samples, rather than the entire batch. The additional <em>noise</em> can be of help here, as you may get an errant data point that kicks off your path down the valley, rather than our divergent model above.</li>
  <li><strong>Normalization of Data</strong> - Normalizing data creates a less <a href="http://www.ou.edu/class/che-design/che5480-13/Optimization%20of%20Chemical%20Processes-Chapter%204.pdf">elliptical contour</a>, and will influence our objective function to have a more <strong>concentric circle</strong> countour (<em>think a circular mountain base instead of an elliptical base!</em>).</li>
</ol>

<p>We will not cover these here for the brevity’s sake, but feel free to explore on your own.</p>

	  ]]></description>
	</item>

	<item>
	  <title>The Gradient: A Visual Descent</title>
	  <link>/sharrington/math-of-machine-learning/the-gradient-a-visual-descent</link>
	  <author>Sean Harrington</author>
	  <pubDate>2017-06-16T09:32:00-04:00</pubDate>
	  <guid>/sharrington/math-of-machine-learning/the-gradient-a-visual-descent</guid>
	  <description><![CDATA[
	     <p><br />
In this post I aim to visually, mathematically and programatically explain <strong>the gradient</strong>, and how its understanding is crucial for <strong>gradient descent</strong>. For those unfamiliar, gradient descent is used in various ML models spanning from logistic regression, to neural nets. Ultimately, gradient descent, and transitively <strong>the gradient</strong>, are the blueprints that guide these models to converge on ideal weights.</p>

<p>In short, The gradient is an n-dimensional generalization of the derivative, which can be applied in any direction.</p>

<p>For a complete understanding of the content in this post, I would suggest a firm understanding of the following topics:</p>

<ol>
  <li>Derivatives and the Chain Rule (<em>Calculus</em>)</li>
  <li>Partial Derivatives (<em>Multivariate Calculus</em>)</li>
  <li>Basic Vector Operations (<em>Linear Algebra</em>)</li>
  <li>Objective Functions (<em>Also called Loss Function</em>)</li>
  <li>Basic Understanding of NumPy</li>
</ol>

<h3 id="the-data">The Data</h3>

<p>Our dataset is made up of South Boston home sales, and consists of 2 very simple column vectors:</p>

<p><script type="math/tex">\hat{y} = HomeValue = \left\langle 550000.00, 600000.00, ... 578000.00\right\rangle^{T}</script>
<script type="math/tex">\hat{x} = LivingArea = \left\langle 990, 1046, ... 1010\right\rangle^{T}</script></p>

<h3 id="the-model">The Model</h3>

<p>We will be learning a simple Linear Regression model, that predicts a home in South Boston’s value (sale price) based solely on the Living Area of the property. We want to fit a line to our data, defined by:</p>

<script type="math/tex; mode=display">\hat{y} = a\hat{x} + b</script>

<p>Our line should overlay our data as follows:
<img src="/assets/images/plotly/south_boston_home_sales_lr.png" alt="S. Boston - LinReg" />
Thus, our main objective is to minimize the distance between each point and our line. We will use <strong>Mean Squared Error</strong> as our objective function, which is a fancy way of saying: <em>“For each point (x, y) in our dataset, take the squared distance between the y-value, and the predicted y-value, and add them all up. Then divide by the number of examples, n.”</em></p>

<p>In mathematical fancy notation, the Mean Squared Error looks as follows:</p>

<script type="math/tex; mode=display">J(a, b) = \frac{1}{n}\sum_{i=1}^{n}(y_{i,actual} - y_{i,predicted})^{2}</script>

<p>Using our above objective function, (\(y_{i,predicted} = ax_{i} + b,\)) we get:</p>

<script type="math/tex; mode=display">J(a, b) = \frac{1}{n}\sum_{i=1}^{n}(y_{i} - (ax_{i} + b))^{2}</script>

<p>Our goal is to minimize this function, and we do so with the gradient.</p>

<p>Let’s begin with the most naive solution to find our minimum: take every combination of \(a\) and \(b\), then find the combination that yields the lowest error \(J(a, b\)).</p>

<p>The problem: What if (rather than \(a\) and \(b\) ) we have 10,000 variables to solve for? We would find ourselves in a permutational nightmare that no computer could handle!</p>

<p>We need a way to gracefully and quickly find our minimum: and we use the gradient to do so.</p>

<h3 id="the-math-finding-the-gradient">The Math: Finding the Gradient</h3>

<p>The gradient is a vector of the partial derivatives of a function. It is the n-dimensional generalization of the derivative, defining the rate of change with respect to all “n” variables. It is usually written as follows:</p>

<script type="math/tex; mode=display">\nabla J(\hat{x}) = \left\langle \frac{\partial{J}}{\partial{x_{0}}}, \frac{\partial{J}}{\partial{x_{1}}}, ... \frac{\partial{J}}{\partial{x_{n}}}\right\rangle^{T}</script>

<p>In our case, we need to find both partial derivatives of \(J(a, b)\) - we use the Chain Rule to do so:</p>

<script type="math/tex; mode=display">\frac{\partial{J}}{\partial{a}} = \frac{1}{n}\sum_{i=1}^{n}x_{i} * 2(y_i - (ax_i + b))</script>

<script type="math/tex; mode=display">\frac{\partial{J}}{\partial{b}} = \frac{1}{n}\sum_{i=1}^{n}2(y_i - (ax_i + b))</script>

<p>Don’t be intimidated by the summation notion when calculating the partials! We are just differentiating the function to the right of the symbol!</p>

<p>Rearranging terms, and distributing terms we get our gradient:</p>

<script type="math/tex; mode=display">\nabla J(a, b) = \left\langle\matrix{
\frac{2}{n}\sum_{i=1}^{n}x_{i}y_{i} - (ax_i^{2} + bx_{i})\cr
\frac{2}{n}\sum_{i=1}^{n}y_{i} - (ax_i + b)
}\right\rangle</script>

<h3 id="the-math-visualizing-the-gradients-significance">The Math: Visualizing the Gradient’s Significance</h3>

<p>If we are standing on the mountain-like curve of our objective function, at point \((a, b)\), we want to find the direction with the greatest rate of negative change.</p>

<p>In other words, we want to find the steepest path down the mountain, because the lower our error (height of our curve), the more accurate our model! Below is the surface of our objective function \(J(a, b)\), and our starting point \((a=340, b=380000)\):</p>

<p><img src="/assets/images/plotly/gd_starting_point.png" alt="Gradient Descent: Starting Point" /></p>

<p>Which direction do we step? It turns out that the direction of the gradient vector, at a given point, will naturally point in the steepest direction!</p>

<p>The gradient will be our guide down the mountain!</p>

<h3 id="the-math-proving-the-gradients-significance">The Math: Proving the Gradient’s Significance</h3>

<p>Here’s where things get a bit “mathy”. Let’s prove why the direction of the gradient vector points in the direction of greatest change.</p>

<p>The rate-of-change of \(J(a, b)\), in a certain direction, \(\vec v\), with respect to distance traveled (\(ds\)), is defined as the gradient dotted with a vector indicating the direction to travel (\(\vec v)\), or:</p>

<script type="math/tex; mode=display">\frac{dJ}{ds} = \nabla (J(a, b))\cdot\vec v</script>

<p><em>All we are saying is that given an arbitrary direction, \(\vec v\), down our mountain, if we travel that direction in \(ds\) units, our error, \(J\), changes at a rate described by the above formula, which depends on the gradient.</em></p>

<p>Using the definition of the dot-product (\(x \cdot y = ||x||\ ||y||\ cos(\theta_{x,y})\)), we can re-write this as the product of each vector’s magnitude, times the cosine of the angle between them:</p>

<script type="math/tex; mode=display">\frac{dJ}{ds} = ||\nabla (J(a, b))||\ ||\vec v||\ cos(\theta)</script>

<p>To maximize the rate of change, we would want to maximize \(cos(\theta)\) by setting  \(\theta = 0\), to get \(1\), the highest possible value of the \(cos()\) function.</p>

<p>If the angle between these two vectors is \(0\), then the two vectors are parallel, therefore:</p>

<script type="math/tex; mode=display">dir(\vec v) = dir(\nabla J)</script>

<p><strong>Our steepest step is in the direction of the gradient!</strong> So we want to travel in the <strong>opposite direction</strong> of the gradient, by multiplying it by \(-1\). If you take anything away from this post, remember this!</p>

<p>Given this fact, at each “step” along our descent, we:</p>

<ol>
  <li>Calculate our gradient at the point \( (a, b) \).
    <ul>
      <li>
        <p style="background-color: #ffff99">We want the negative gradient, so we travel downhill and not uphill!</p>
      </li>
    </ul>
  </li>
  <li>Multiply the gradient by a scalar, (also called a <strong>learning rate</strong>, denoted by \(\eta\))
    <ul>
      <li>The learning rate is what we call a <strong>hyperparameter</strong>, and can be tweaked to ensure convergence. <a href="/sharrington/math-of-machine-learning/gradient-descent-learning-rate-too-high">Set too high or low, the model might converge!</a></li>
    </ul>
  </li>
  <li>Update our weights using our multiplied gradient:
<script type="math/tex">\left\langle a, b \right\rangle += \eta \nabla J</script></li>
  <li>Terminate if we have converged!</li>
</ol>

<h3 id="the-math-implementing-gradient-descent">The Math: Implementing Gradient Descent</h3>

<p>The equivalent code, implemented using Python and NumPy looks as follows:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c"># I've experimented w/ Python3.6 unicode symbols in this snippet</span>
<span class="k">def</span> <span class="nf">gradient_descent_mse</span><span class="p">(</span><span class="n">x_vec</span><span class="p">,</span> <span class="n">y_vec</span><span class="p">):</span>                                                 
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_vec</span><span class="p">)</span>                                                                      
    <span class="c"># Define Partials &amp; Objective Function (MSE)</span>
    <span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>                                                                    
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>                                   
    <span class="k">def</span> <span class="nf">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>                                                                
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>                                           
    <span class="k">def</span> <span class="nf">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>                                                                
       <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x_vec</span> <span class="o">*</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="n">x_vec</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">))</span> <span class="o">/</span> <span class="n">n</span>                                     
    <span class="c"># Initialize our weights                                                        </span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">350</span>                                                                       
    <span class="n">b</span> <span class="o">=</span> <span class="mi">380000</span>                                                                      
    <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Infinity</span>                                                                
    <span class="n">j</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>                                                                     
    <span class="c"># Convergence Conditions                                                        </span>
    <span class="err">δ</span> <span class="o">=</span> <span class="mi">1</span>                                                                           
    <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">150</span>                                                             
    <span class="c"># Learning Rates (separate rate for each partial)</span>
    <span class="err">ŋ</span><span class="n">_b</span> <span class="o">=</span> <span class="o">.</span><span class="mi">15</span>                                                                       
    <span class="err">ŋ</span><span class="n">_a</span> <span class="o">=</span> <span class="o">.</span><span class="mo">00000005</span>                                                                 
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>                                                                           
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="err">Δ</span><span class="n">j</span><span class="p">)</span> <span class="o">&gt;</span> <span class="err">δ</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span>                                       
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>                                                                      
        <span class="c"># Find the gradient at the point (a, b)                                     </span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">dJ_da</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">dJ_db</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)]</span>                                                     
        <span class="c"># Multiply each partial deriv by its learning rate                          </span>
        <span class="err">Δ</span><span class="n">a</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_a</span>                                                         
        <span class="err">Δ</span><span class="n">b</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="o">-</span><span class="err">ŋ</span><span class="n">_b</span>                                                         
        <span class="c"># Update our weights                                                        </span>
        <span class="n">a</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">a</span>                                                                     
        <span class="n">b</span> <span class="o">+=</span> <span class="err">Δ</span><span class="n">b</span>                                                                     
        <span class="c"># Update the error at each iteration                                        </span>
        <span class="n">j_new</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>                                                             
        <span class="err">Δ</span><span class="n">j</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="n">j_new</span>                                                              
        <span class="n">j</span> <span class="o">=</span> <span class="n">j_new</span> 
    <span class="k">print</span><span class="p">(</span><span class="s">"y = {0}x + {1}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">)))</span>
</code></pre>
</div>

<p>We stop our descent when we have maxed out on iterations, or when we see (very small) decreases in the error calculated by \(J\).</p>

<p>To visualize our graceful descent, the following surface plots 150 iterations of our Gradient Descent, and shows us converging on \( a \approx 148, b \approx 380000\).</p>

<p><img src="/assets/images/plotly/gd_ending_point.png" alt="Gradient Descent: Finish" /></p>

<p>You can clearly see the “path down the mountain”, and visually grasp how the algorithm actually converges.</p>

<p>Our final “line of best fit” after 150 iterations, superimposed atop our data, and the closed-form solution, looks almost identical to our closed-form solution (<em>if we had iterated 100 more times, the lines would have been superimposed, and indistinguishable!</em>):</p>

<p><img src="/assets/images/plotly/south_boston_home_sales.png" alt="S. Boston - LinReg vs Gradient Descent" /></p>

<h3 id="conclusion">Conclusion</h3>

<p>In our example here, we have mathematically, programatically, and visually proved the effectiveness of <strong>Gradient Descent</strong> in minimizing Objective Functions. Perhaps more importantly, we have shed light on <strong>the gradient</strong>, and why it is such a graceful solution to our error minimization.</p>

<p>There are many flavors of Gradient Descent, other than the (naive) implementation provided above. Stochastic Gradient Descent (SGD) is the most commonly used algorithm for minimizations, and is found everywhere from Logistic Regression Classifiers, to Deep Learning and Neural Nets.</p>

<p>Some of you may point out that the above example is trivial! There is a closed solution to this problem of Least Squares Regression! We’ve used this example for simplicity’s sake to introduce <strong>The Gradient</strong> and <strong>Gradient Descent</strong>. <a href="http://thelaziestprogrammer.comsharrington/math-of-machine-learning/solving-logreg-newtons-method">This post on solving Logistic Regression iteratively</a>, addresses a more practical problem: there is no closed form solution to Logistic Regression!</p>


	  ]]></description>
	</item>


</channel>
</rss>
